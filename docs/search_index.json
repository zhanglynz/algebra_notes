[
["index.html", "Linear Algebra Notes Preface", " Linear Algebra Notes Lingyun Zhang 2021-11-07 Preface To be written. "],
["looking-into-matrices.html", "1 Looking into matrices 1.1 Column and row vectors 1.2 Definition of a matrix 1.3 Transpose 1.4 A matrix times a vector 1.5 Column space of \\({\\boldsymbol A}_{m\\times n}\\) 1.6 Rank of a matrix 1.7 Representing a matrix as CR and CMR 1.8 Matrix multiplication", " 1 Looking into matrices If not specifically stated, all the numbers that we use are real. 1.1 Column and row vectors We use bold lowercase letters to denote vectors. All the vectors are column vectors; if we need row vector, then we use transpose \\(T\\). Examples: \\({\\boldsymbol a}^T = (1,\\ 2,\\ 3)\\) \\({\\boldsymbol b}^T= (1,\\ 0,\\ 0)\\) \\({\\boldsymbol x}^T= (1/2,\\ 1/3,\\ 1/4)\\) 1.2 Definition of a matrix Let \\(m\\) and \\(n\\) be positive integers. \\({\\boldsymbol A}_{m\\times n}\\) is matrix—it’s an \\(m\\)-by-\\(n\\) table, containing \\(m\\times n\\) numbers. Element picture: \\[ {\\boldsymbol A}_{m\\times n} = \\left[a_{ij}\\right]. \\] Column picture: \\[ {\\boldsymbol A}_{m\\times n} = \\left[ {\\boldsymbol c}_1,\\ \\ldots,\\ {\\boldsymbol c}_n \\right], \\] where \\({\\boldsymbol c}_i\\in {\\mathbf R}^m\\) for \\(i=1,\\ldots,\\ n.\\) Row picture: \\[ {\\boldsymbol A}_{m\\times n} = \\left[ \\begin{array}{c} {\\boldsymbol r}_1^T\\\\ \\vdots\\\\ {\\boldsymbol r}_m^T \\end{array} \\right], \\] where \\({\\boldsymbol r}_i\\in {\\mathbf R}^n\\) for \\(i=1, \\ldots, m\\). 1.3 Transpose \\[ {\\boldsymbol A}^T= \\left[ \\begin{array}{c} {\\boldsymbol c}_1^T\\\\ \\vdots\\\\ {\\boldsymbol c}_n^T \\end{array} \\right]= \\left[ {\\boldsymbol r}_1,\\ \\ldots,\\ {\\boldsymbol r}_m \\right], \\] which is an \\(n\\)-by-\\(m\\) matrix. 1.4 A matrix times a vector By row: it’s inner product of rows; by column: it’s linear combination of columns. For example \\[ \\left[ \\begin{array}{ccc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; 6 \\end{array} \\right] \\left[ \\begin{array}{c} x_1\\\\ x_2 \\end{array} \\right]=\\left[ \\begin{array}{r} x_1+2x_2\\\\ 3x_1+4x_2\\\\ 5x_1+6x_2 \\end{array} \\right], \\] or \\[ \\left[ \\begin{array}{ccc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; 6 \\end{array} \\right] \\left[ \\begin{array}{c} x_1\\\\ x_2 \\end{array} \\right]=x_1\\left[ \\begin{array}{c} 1\\\\ 3\\\\ 5 \\end{array} \\right]+ x_2\\left[ \\begin{array}{c} 2\\\\ 4\\\\ 6 \\end{array} \\right]. \\] 1.5 Column space of \\({\\boldsymbol A}_{m\\times n}\\) The column space is \\[ Col({\\boldsymbol A}_{m\\times n})= \\left\\{ {\\boldsymbol y}: {\\boldsymbol y} = {\\boldsymbol A}{\\boldsymbol x},\\ \\hbox{where}\\ {\\boldsymbol x}\\in {\\mathbf R}^n \\right\\}. \\] A bunch of vectors are linearly independent, if there is only one way to represent the zero vector as a linear combination of these vectors; or, equivalently, if any one of them cannot be represented as a linear combination of the others. The basis of \\(Col({\\boldsymbol A})\\) consists of a subset of the columns of \\({\\boldsymbol A}\\), such that a) these columns are linearly independent, b) any a column of \\({\\boldsymbol A}\\) is a linear combination of these columns. 1.6 Rank of a matrix The rank of a matrix is the dimension of its column space. To find rank of a matrix, we need to find a basis of its column space, for this, an algorithm is: if column 1 is not all zeros, keep it; if column 2 is not a multiple of column 1, keep it; if column 3 is not a linear combination of columns 1 and 2, keep it. Continue this way until find all the columns that forms a basis. It can be shown that \\[ \\hbox{rank}({\\boldsymbol A})= \\hbox{rank}({\\boldsymbol A}^T)= \\hbox{rank}({\\boldsymbol A}^T{\\boldsymbol A})=\\hbox{rank}({\\boldsymbol A}{\\boldsymbol A}^T). \\] 1.7 Representing a matrix as CR and CMR The letters in CR and CMR stand for Column, Row and Mixing. In the CR and CMR expressions, columns of matrix \\({\\boldsymbol C}\\) is a basis of \\(Col({\\boldsymbol A})\\) and they are directly from \\({\\boldsymbol A}\\). In the CMR expression, transpose of rows of matrix \\({\\boldsymbol R}\\) is a basis of row space of \\({\\boldsymbol A}\\) and they are directly from the rows of \\({\\boldsymbol A}\\). The CR and CMR expressions both show that column and row ranks of a matrix are the same. Example: CR expression \\[ \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9 \\end{array} \\right]= \\left[ \\begin{array}{cc} 1 &amp; 2\\\\ 4 &amp; 5\\\\ 7 &amp; 8 \\end{array} \\right]\\left[ \\begin{array}{ccr} 1 &amp; 0 &amp; -1\\\\ 0 &amp; 1 &amp; 2 \\end{array} \\right]. \\] Example: CMR expression \\[ \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9 \\end{array} \\right]= \\left[ \\begin{array}{cc} 1 &amp; 2\\\\ 4 &amp; 5\\\\ 7 &amp; 8 \\end{array} \\right]\\left[ \\begin{array}{rr} -\\frac{5}{3} &amp; \\frac{2}{3}\\\\ \\frac{4}{3} &amp; -\\frac{1}{3} \\end{array} \\right]\\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{array} \\right]. \\] 1.8 Matrix multiplication Let \\[ {\\boldsymbol A}_{m\\times n}=[a_{ij}], \\] and \\[ {\\boldsymbol B}_{n\\times p}=[b_{ij}]. \\] Two pictures of matrix multiplication: row times column \\[ {\\boldsymbol AB}=[c_{ij}], \\] where \\[ c_{ij}=\\sum_{k=1}^n a_{ik}b_{kj}. \\] column times row \\[ {\\boldsymbol AB}=\\left[{\\boldsymbol a}_1, \\ldots, {\\boldsymbol a}_n\\right]\\left[ \\begin{array}{c} {\\boldsymbol b}_1^T\\\\ \\vdots\\\\ {\\boldsymbol b}_n^T \\end{array} \\right]=\\sum_{i=1}^n{\\boldsymbol a}_i{\\boldsymbol b}_i^T. \\] Note that \\[ \\hbox{rank}({\\boldsymbol a}_i{\\boldsymbol b}_i^T)\\le 1,\\ \\hbox{for}\\ i=1, \\ldots, n. \\] Example: \\({\\boldsymbol S}_{n\\times n}\\) is a symmetric matrix. Then \\[ \\begin{align} {\\boldsymbol S}&amp;={\\boldsymbol Q} \\left[ \\begin{array}{ccc} \\lambda_1 &amp; &amp;\\\\ &amp; \\ddots &amp;\\\\ &amp;&amp;\\lambda_n \\end{array} \\right]{\\boldsymbol Q}^T\\\\ &amp;=\\left[ \\lambda_1{\\boldsymbol q}_1, \\ldots, \\lambda_n{\\boldsymbol q}_n\\right]\\left[ \\begin{array}{c} {\\boldsymbol q}_1^T,\\\\ \\vdots,\\\\ {\\boldsymbol q}_n^T \\end{array} \\right]\\\\ &amp;=\\sum_{i=1}^n\\lambda_i{\\boldsymbol q}_i{\\boldsymbol q}_i^T, \\end{align} \\] where \\(\\lambda_i\\) and \\({\\boldsymbol q}_i\\) are the \\(i\\)th eigenvalue and eigenvector of \\({\\boldsymbol S}\\), respectively. "],
["the-four-subspaces.html", "2 The four subspaces 2.1 Column and null spaces 2.2 Ranks inequalities", " 2 The four subspaces 2.1 Column and null spaces Given matrix \\({\\boldsymbol A}_{m\\times n}\\), we have four subspaces: column space \\(col({\\boldsymbol A})\\) row space \\(col({\\boldsymbol A}^T)\\) null space \\(null({\\boldsymbol A})\\) left null space \\(null({\\boldsymbol A}^T)\\) Suppose that \\[ \\hbox{rank}({\\boldsymbol A})=r, \\] then \\[ col({\\boldsymbol A}) \\oplus null({\\boldsymbol A}^T)={\\boldsymbol R}^m, \\] and \\[ col({\\boldsymbol A}^T) \\oplus null({\\boldsymbol A})={\\boldsymbol R}^n, \\] where \\(\\oplus\\) is for direct sum. 2.2 Ranks inequalities \\[ \\hbox{rank}({\\boldsymbol A}{\\boldsymbol B})\\le \\min\\{\\hbox{rank}({\\boldsymbol A}),\\ \\hbox{rank}({\\boldsymbol B})\\} \\] \\[ \\hbox{rank}({\\boldsymbol A}+{\\boldsymbol B})\\le \\hbox{rank}({\\boldsymbol A})+ \\hbox{rank}({\\boldsymbol B}) \\] \\[ \\hbox{rank}({\\boldsymbol A})=\\hbox{rank}({\\boldsymbol A}^T)=\\hbox{rank}({\\boldsymbol A}{\\boldsymbol A}^T)=\\hbox{rank}({\\boldsymbol A}^T{\\boldsymbol A}) \\] Given \\({\\boldsymbol A}_{m\\times r}\\) and \\({\\boldsymbol B}_{r\\times n}\\), if \\[ \\hbox{rank}({\\boldsymbol A})=\\hbox{rank}({\\boldsymbol B})=r, \\] then \\[ \\hbox{rank}({\\boldsymbol A}{\\boldsymbol B})=r. \\] "],
["elimination-and-lu.html", "3 Elimination and LU 3.1 Elimination 3.2 About permutation matrices", " 3 Elimination and LU 3.1 Elimination To solve \\[ {\\boldsymbol Ax}={\\boldsymbol b}, \\] we use the (Gaussian) elimination method. Elimination means that a) we “knock out” a variable each time, and finally we have an equation like \\[ kx_n=b^\\ast, \\] from which we have \\[ x_n=\\frac{b^\\ast}{k}; \\] (b) working backwards to have \\(x_{n-1},\\ldots, x_1\\). We do elementary row operation in the elimination process. Example: Express \\[ \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9 \\end{array} \\right] \\] as LU, i.e. a product of lower and upper triangular matrices. Solution: \\[\\begin{align} &amp;\\left[ \\begin{array}{ccc|ccc} 1 &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\\\ 4 &amp; 5 &amp; 6 &amp; 0 &amp; 1 &amp; 0\\\\ 7 &amp; 8 &amp; 9 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right]\\\\ \\longrightarrow &amp; \\left[ \\begin{array}{crr|rcc} 1 &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; -3 &amp; -6 &amp; -4 &amp; 1 &amp; 0\\\\ 0 &amp; -6 &amp; -12 &amp; -7 &amp; 0 &amp; 1 \\end{array} \\right]\\\\ \\longrightarrow &amp; \\left[ \\begin{array}{crr|rrc} 1 &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; -3 &amp; -6 &amp; -4 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 \\end{array} \\right]=\\left[{\\boldsymbol U} | {\\boldsymbol L}^{-1}\\right]. \\end{align}\\] Next we find \\({\\boldsymbol L}\\). \\[\\begin{align} &amp;\\left[ \\begin{array}{rrc|ccc} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ -4 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; -2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right]\\\\ \\longrightarrow &amp; \\left[ \\begin{array}{crc|rcc} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 4 &amp; 1 &amp; 0\\\\ 0 &amp; -2 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\end{array} \\right]\\\\ \\longrightarrow &amp; \\left[ \\begin{array}{ccc|rrc} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 4 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 2 &amp; 1 \\end{array} \\right]=\\left[{\\boldsymbol I} | {\\boldsymbol L}\\right]. \\end{align}\\] 3.2 About permutation matrices There are \\(n!\\) (\\(n\\) factorial) permutation matrices of size \\(n\\). The inverse of every permutation matrix is its transpose (because permutation matrix is orthogonal matrix.) "],
["orthogonal-matrices-and-subspaces.html", "4 Orthogonal matrices and subspaces 4.1 Definition 4.2 Hadamard matrices 4.3 Orthogonal basis 4.4 Projection matrix 4.5 In \\({\\boldsymbol R}^2\\) 4.6 Orthogonal basis (or orthogonal axes) in \\({\\boldsymbol R}^n\\) 4.7 Householder matrices", " 4 Orthogonal matrices and subspaces 4.1 Definition \\({\\boldsymbol Q}\\) is an orthogonal matrix, if \\({\\boldsymbol Q}\\) is square; \\({\\boldsymbol Q}^T{\\boldsymbol Q}={\\boldsymbol I}\\Longleftrightarrow {\\boldsymbol Q}{\\boldsymbol Q}^T={\\boldsymbol I}\\). 4.2 Hadamard matrices The Wikipedia link is: https://en.wikipedia.org/wiki/Hadamard_matrix Hadamard conjecture: There is an \\(n\\)-by-\\(n\\) \\(\\pm 1\\) matrix with orthogonal columns whenever \\(4\\) divides \\(n\\). 4.3 Orthogonal basis Every subspace of \\({\\boldsymbol R}^n\\) has an orthogonal basis (this can be done by using Gram-Schmidt procedure) The row space of \\({\\boldsymbol A}\\) is orthogonal to the null space of \\({\\boldsymbol A}\\). The column space of \\({\\boldsymbol A}\\) is orthogonal to the null space of \\({\\boldsymbol A}^T\\). SVD (Singular Value Decomposition) finds orthogonal bases \\({\\boldsymbol v}_1, \\ldots, {\\boldsymbol v}_r\\) for the row space of \\({\\boldsymbol A}\\) and \\({\\boldsymbol u}_1, \\ldots, {\\boldsymbol u}_r\\) for the column space such that \\[ {\\boldsymbol Av}_i=\\sigma_i{\\boldsymbol u}_i,\\ \\hbox{for}\\ i=1, \\ldots, r, \\] where \\(r\\) is the rank of \\({\\boldsymbol A}\\). That is, for the bases from the SVD, multiplying by \\({\\boldsymbol A}\\) takes an orthogonal basis of \\({\\boldsymbol v}\\)’s to an orthogonal basis of \\({\\boldsymbol u}\\)’s. 4.4 Projection matrix If \\({\\boldsymbol P}^2={\\boldsymbol P}={\\boldsymbol P}^T\\), then \\({\\boldsymbol Pb}\\) is the orthogonal projection of \\({\\boldsymbol b}\\) onto the column space of \\({\\boldsymbol P}\\). 4.5 In \\({\\boldsymbol R}^2\\) Rotation matrix (rotation through an angle \\(\\theta\\)) \\[ {\\boldsymbol Q}_{rot}=\\left[ \\begin{array}{rr} \\cos\\theta &amp; -\\sin\\theta\\\\ \\sin\\theta &amp; \\cos\\theta \\end{array} \\right] \\] Reflection matrix (reflection across \\(\\theta/2\\)-line) \\[ {\\boldsymbol Q}_{ref}=\\left[ \\begin{array}{rr} \\cos\\theta &amp; \\sin\\theta\\\\ \\sin\\theta &amp; -\\cos\\theta \\end{array} \\right] \\] 4.6 Orthogonal basis (or orthogonal axes) in \\({\\boldsymbol R}^n\\) Suppose that orthogonal matrix \\({\\boldsymbol Q}\\) has columns \\({\\boldsymbol q}_1,\\ldots, {\\boldsymbol q}_n\\). Those unit vectors are a basis of space \\({\\boldsymbol R}^n\\). Every vector in \\({\\boldsymbol R}^n\\) can be expressed as a linear combination of \\({\\boldsymbol q}\\)’s: \\[ v=\\sum_{i=1}^n c_i{\\boldsymbol q}_i, \\] where \\[ c_i = {\\boldsymbol q}^T{\\boldsymbol v}. \\] Note that when basis vectors are orthogonal, each coefficient \\(c_1\\) to \\(c_n\\) can be found independently. 4.7 Householder matrices \\[ {\\boldsymbol H}_n={\\boldsymbol I}_n - 2{\\boldsymbol u}{\\boldsymbol u}^T, \\] where \\({\\boldsymbol u}\\in {\\boldsymbol R}^n\\) and \\(||{\\boldsymbol u}||=1\\). It can be shown that \\[ {\\boldsymbol H}_n {\\boldsymbol u} = - {\\boldsymbol u}, \\] and \\[ {\\boldsymbol H}_n {\\boldsymbol w} = {\\boldsymbol w}, \\] if \\({\\boldsymbol w}\\) is perpendicular to \\({\\boldsymbol u}\\). "],
["eigenvalues-and-eigenvectors.html", "5 Eigenvalues and eigenvectors 5.1 Basics 5.2 Similar matrices 5.3 Homogeneous linear difference equation", " 5 Eigenvalues and eigenvectors 5.1 Basics \\({\\boldsymbol A}\\) is an \\(n\\)-by-\\(n\\) matrix. If \\[ {\\boldsymbol A\\xi}=\\lambda {\\boldsymbol \\xi}, \\] then \\(\\lambda\\) is an eigenvalue and \\({\\boldsymbol \\xi}\\) is an eigenvector of \\({\\boldsymbol A}\\). Notice that \\[ {\\boldsymbol A}^k{\\boldsymbol \\xi}=\\lambda^k {\\boldsymbol \\xi}, \\] that is, \\(\\lambda^k\\) is eigenvalue and \\({\\boldsymbol \\xi}\\) is eigenvector of \\({\\boldsymbol A}^k\\). Suppose that \\(\\lambda_1,\\ldots, \\lambda_n\\) are eigenvalues of \\({\\boldsymbol A}\\). Three facts: \\[ \\hbox{trace}({\\boldsymbol A})=\\sum_{i=1}^n \\lambda_i. \\] \\[ \\hbox{det}({\\boldsymbol A})=\\prod_{i=1}^n \\lambda_i. \\] The eigenvectors of a real matrix \\({\\boldsymbol A}\\) are orthogonal if and only if \\[ {\\boldsymbol A}^T{\\boldsymbol A}={\\boldsymbol A}{\\boldsymbol A}^T. \\] Quick proof for Facts 1 and 2. The characteristic polynomial is \\[ |\\lambda {\\boldsymbol I} - {\\boldsymbol A}|=\\prod_{i=1}^n (\\lambda - \\lambda_i). \\] In the LHS the coefficient of \\(\\lambda^{n-1}\\) is \\(-\\hbox{trace}({\\boldsymbol A})\\), and in the RHS the coefficient of \\(\\lambda^{n-1}\\) is \\(-\\sum_{i=1}^n \\lambda_i\\). Thus we have Fact 1. Let \\(\\lambda\\) be 0 in the characteristic polynomial, we have \\[ (-)^n|{\\boldsymbol A}| = (-1)^n \\prod_{i=1}^n \\lambda_i, \\] which implies Fact 2. 5.2 Similar matrices For every invertible matrix \\({\\boldsymbol B}\\), the eigenvalues of \\({\\boldsymbol BAB}^{-1}\\) are the same as eigenvalues of \\({\\boldsymbol A}\\). The eigenvector \\({\\boldsymbol x}\\) of \\({\\boldsymbol A}\\) are multiplied by \\({\\boldsymbol B}\\) to give eigenvectors of \\({\\boldsymbol BAB}^{-1}\\). The matrices \\({\\boldsymbol BAB}^{-1}\\) (for every invertible \\({\\boldsymbol B}\\)) are similar to \\({\\boldsymbol A}\\). \\({\\boldsymbol A}\\) is an \\(n\\)-by-\\(n\\) matrix. \\({\\boldsymbol A}\\) is similar to a diagonal matrix if and only if it has \\(n\\) independent eigenvectors, \\({\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\). \\[\\begin{align} {\\boldsymbol A}\\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right]=&amp;\\left[{\\boldsymbol A}{\\boldsymbol x}_1, \\ldots, {\\boldsymbol A}{\\boldsymbol x}_n\\right]\\\\ =&amp;\\left[\\lambda_1{\\boldsymbol x}_1, \\ldots, \\lambda_n{\\boldsymbol x}_n\\right]\\\\ =&amp;\\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right]\\left[ \\begin{array}{ccc} \\lambda_1 &amp; &amp;\\\\ &amp; \\ddots &amp;\\\\ &amp;&amp; \\lambda_n \\end{array} \\right], \\end{align}\\] which implies \\[ {\\boldsymbol A}=\\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right] \\left[ \\begin{array}{ccc} \\lambda_1 &amp; &amp;\\\\ &amp; \\ddots &amp;\\\\ &amp;&amp; \\lambda_n \\end{array} \\right] \\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right]^{-1} \\] and \\[ {\\boldsymbol A}^k=\\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right] \\left[ \\begin{array}{ccc} \\lambda_1^k &amp; &amp;\\\\ &amp; \\ddots &amp;\\\\ &amp;&amp; \\lambda_n^k \\end{array} \\right] \\left[{\\boldsymbol x}_1, \\ldots, {\\boldsymbol x}_n\\right]^{-1}. \\] Geometric multiplicity (= GM) counts the independent eigenvectors for \\(\\lambda\\); we look at the dimension of null space of \\(\\lambda{\\boldsymbol I}-{\\boldsymbol A}\\). Algebraic multiplicity (= AM) counts the repetitions of \\(\\lambda\\) among eigenvalues; we look at the roots of \\({\\hbox{det}(\\lambda{\\boldsymbol I}-{\\boldsymbol A})}=0.\\) The shortage of eigenvectors when \\(\\hbox{GM}&lt;\\hbox{AM}\\) means that \\({\\boldsymbol A}\\) is not diagonalizable (or in other words, it’s not similar to a diagonal matrix). 5.3 Homogeneous linear difference equation Let’s have an example. For integer sequence \\(\\{a_n\\}\\), \\(a_0=a_1=a_2=1\\) and if \\(n\\ge 3\\) \\[ a_n = pa_{n-1} + qa_{n-2}+ra_{n-3}. \\] Find an expression of \\(a_n\\). Solution: \\[\\begin{align} \\left[ \\begin{array}{l} a_{n+2}\\\\ a_{n+1}\\\\ a_n \\end{array} \\right]=&amp; \\left[ \\begin{array}{ccc} p &amp; q &amp; r\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right]\\left[ \\begin{array}{l} a_{n+1}\\\\ a_{n}\\\\ a_{n-1} \\end{array} \\right]\\\\ =&amp;\\left[ \\begin{array}{ccc} p &amp; q &amp; r\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right]^2\\left[ \\begin{array}{l} a_{n}\\\\ a_{n-1}\\\\ a_{n-2} \\end{array} \\right]\\\\ \\vdots&amp;\\\\ =&amp;\\left[ \\begin{array}{ccc} p &amp; q &amp; r\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right]^n\\left[ \\begin{array}{l} a_{2}\\\\ a_{1}\\\\ a_{0} \\end{array} \\right]:={\\boldsymbol A}^n\\left[ \\begin{array}{l} a_{2}\\\\ a_{1}\\\\ a_{0} \\end{array} \\right]. \\end{align}\\] Note that if \\({\\boldsymbol A}\\) is diagonalizable, then it’s easy to compute \\({\\boldsymbol A}^n\\), and we can find a closed-form expression of \\(a_n\\). Also notice that \\[\\begin{align} |\\lambda {\\boldsymbol I}-{\\boldsymbol A}|=&amp; \\begin{array}{|rrr|} \\lambda-p &amp; -q &amp; -r\\\\ -1 &amp; \\lambda &amp; 0\\\\ 0 &amp; -1 &amp; \\lambda \\end{array}\\\\ =&amp; -r \\begin{array}{|rr|} -1 &amp; \\lambda\\\\ 0 &amp; -1 \\end{array}+\\lambda \\begin{array}{|rr|} \\lambda-p &amp; -q\\\\ -1 &amp; \\lambda \\end{array}\\\\ =&amp;-r + \\lambda\\left[(\\lambda-p)\\lambda - q\\right]\\\\ =&amp;-r+\\lambda\\left[\\lambda^2-p\\lambda-q\\right]\\\\ =&amp;\\lambda^3-p\\lambda^2-q\\lambda-r. \\end{align}\\] "],
["symmetric-positive-definite-matrices.html", "6 Symmetric positive definite matrices 6.1 Spectral theorem 6.2 Positive definite matrices 6.3 Application", " 6 Symmetric positive definite matrices 6.1 Spectral theorem Two important facts: For an \\(n\\)-by-\\(n\\) symmetric matrix \\({\\boldsymbol S}\\), all its \\(n\\) eigenvalues are real numbers; it has \\(n\\) independent eigenvectors and they can be made orthonormal (perpendicular to each other and each of them has length 1); eigenvectors that belong to different eigenvalues are orthogonal. Spectral Theorem: Every real symmetric matrix has the form \\[ {\\boldsymbol S}={\\boldsymbol Q\\Lambda Q}^T, \\] where \\({\\boldsymbol \\Lambda}\\) is a diagonal matrix with eigenvalues of \\({\\boldsymbol S}\\) on the diagonal positions, \\({\\boldsymbol Q}^T{\\boldsymbol Q}={\\boldsymbol I}\\) and columns of \\({\\boldsymbol Q}\\) are eigenvectors of \\({\\boldsymbol S}\\). 6.2 Positive definite matrices Definitions: {S} is a symmetric real matrix. If for any non-zero vector \\({\\boldsymbol x}\\in {\\boldsymbol R}^n\\), \\[ {\\boldsymbol x}^T{\\boldsymbol Sx}&gt;0, \\] then we say \\({\\boldsymbol S}\\) is a positive definite matrix. Testing if \\({\\boldsymbol S}\\) is positive definite: definition; all eigenvalues of \\({\\boldsymbol S}\\) are positive; all the leading determinants of \\({\\boldsymbol S}\\) are positive; \\({\\boldsymbol S}={\\boldsymbol A}^T{\\boldsymbol A}\\) for a matrix \\({\\boldsymbol A}\\) with independent columns. An inequality: Suppose that \\({\\boldsymbol S}\\) is positive definite with eigenvalues \\(\\lambda_1\\ge \\lambda_2\\ge \\ldots \\ge \\lambda_n\\). Then for any \\({\\boldsymbol x}\\in {\\boldsymbol R}^n\\), \\[ {\\boldsymbol x}^T{\\boldsymbol Sx}\\le \\lambda_1 {\\boldsymbol x}^T{\\boldsymbol x}. \\] (Hint: RHS minus LHS is \\({\\boldsymbol x}^T(\\lambda_1{\\boldsymbol I- S}){\\boldsymbol x}\\), and the eigenvalues of \\(\\lambda_1{\\boldsymbol I- S}\\) are all non-negative.) 6.3 Application Minimize \\(f(x, y)\\). Minimum is at \\(x_0,\\ y_0\\), which are solutions to \\[ \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial y}=0, \\] and \\[ \\left[ \\begin{array}{cc} \\partial^2f/\\partial x^2 &amp; \\partial^2f/\\partial x \\partial y\\\\ \\partial^2f/\\partial x \\partial y &amp; \\partial^2f/\\partial y^2 \\end{array} \\right] \\] is positive definite at \\(x_0,\\ y_0\\). Gradient descent: Each step takes the steepest direction toward the target. Machine learning produces “loss functions” with hundreds of thousands of variables. They measure the error—which we minimize. But computing all the second derivatives is completely impossible. We use first derivatives to tell us a direction to move—the error drops fastest in the steepest direction. Then we take another descent step in a new direction. This is the central computation in least squares and neural nets and deep learning. "],
["singular-value-decomposition-svd.html", "7 Singular value decomposition (SVD) 7.1 Key ideas 7.2 Proof of SVD (a sketch) 7.3 The first singular vector", " 7 Singular value decomposition (SVD) 7.1 Key ideas If matrix \\({\\boldsymbol A}_{m\\times n}\\) is not square, then \\({\\boldsymbol Ax}=\\lambda{\\boldsymbol x}\\) is impossible and eigenvectors fail (the left side in \\({\\boldsymbol R}^m\\), the right side in \\({\\boldsymbol R}^n\\).) The key idea is that we need two sets of singular vectors, the \\({\\boldsymbol u}\\)’s and \\({\\boldsymbol v}\\)’s. For a real \\(m\\)-by-\\(n\\) matrix, the \\(n\\) right singular vectors \\({\\boldsymbol v}_1,\\ldots, {\\boldsymbol v}_n\\) are orthogonal in \\({\\boldsymbol R}^n\\); the \\(m\\) left singular vectors \\({\\boldsymbol u}_1,\\ldots, {\\boldsymbol u}_m\\) are orthogonal in \\({\\boldsymbol R}^m\\). The right and left singular vectors are connected in the following way: \\[ \\left\\{ \\begin{array}{l} {\\boldsymbol Av}_1=\\sigma_1{\\boldsymbol u}_1, \\ldots, {\\boldsymbol Av}_r=\\sigma_r{\\boldsymbol u}_r,\\\\ {\\boldsymbol Av}_{r+1}=\\ldots {\\boldsymbol Av}_n={\\boldsymbol 0}, \\end{array} \\right. \\] where \\(r\\) is the rank of \\({\\boldsymbol A}\\), \\[ \\sigma_1\\ge \\sigma_2\\ge \\cdots \\ge \\sigma_r&gt;0. \\] The last \\(n-r\\) \\({\\boldsymbol v}\\)’s are in the null space of \\({\\boldsymbol A}\\), and the last \\(m-r\\) \\({\\boldsymbol u}\\)’s are in the null space of \\({\\boldsymbol A}^T\\). The matrix format is: \\[ {\\boldsymbol AV}={\\boldsymbol U\\Sigma}, \\] i.e. \\[ {\\boldsymbol A}\\left[ {\\boldsymbol v}_1, \\ldots, {\\boldsymbol v}_r, \\ldots, {\\boldsymbol v}_n \\right] = \\left[ {\\boldsymbol u}_1, \\ldots, {\\boldsymbol u}_r, \\ldots, {\\boldsymbol u}_n \\right]=\\left[ \\begin{array}{c|c} \\begin{array}{ccc} \\sigma_1 &amp; &amp;\\\\ &amp; \\vdots &amp;\\\\ &amp;&amp; \\sigma_r \\end{array} &amp; {\\boldsymbol 0}\\\\ \\hline\\\\ {\\boldsymbol 0} &amp; {\\boldsymbol 0} \\end{array} \\right]. \\] The singular value decomposition of \\({\\boldsymbol A}\\) is \\[ {\\boldsymbol A}={\\boldsymbol U\\Sigma V}^T=\\sum_{i=1}^r \\sigma_i {\\boldsymbol u}_i{\\boldsymbol v}_i^T. \\] 7.2 Proof of SVD (a sketch) 7.3 The first singular vector "]
]
